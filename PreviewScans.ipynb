{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview the Stickleback scans\n",
    "\n",
    "The initial state of this notebook is a simple copy of [the notebook generated for the EAWAG Cichlids project](https://github.com/habi/EAWAG/blob/main/DisplayFishes.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are used to set up the whole notebook.\n",
    "They load needed libraries and set some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "# This is loaded as a submodule to alleviate excessive copy-pasting between *all* projects we do\n",
    "# See https://github.com/habi/BrukerSkyScanLogfileRuminator for details on its inner workings\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code linting\n",
    "# %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "# We use the fast internal SSD for speed reasons\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the (tomographic) data can reside on different drives we set a folder to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        BasePath = os.path.join('N:\\\\')\n",
    "Root = os.path.join(BasePath, 'IEE Stickleback')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up, actually start to load/ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files present on disk\n",
    "# Using os.walk is way faster than using recursive glob.glob\n",
    "# Not sorting the found logfiles is also making it quicker\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a (small) sampler of the loaded data as a first check\n",
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for samples which are not yet reconstructed\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row.Folder:\n",
    "        if 'TScopy' not in row.Folder and 'PR' not in row.Folder:\n",
    "            # If there's nothing with 'rec*' on the same level, then tell us\n",
    "            if not glob.glob(row.Folder.replace('proj', '*rec*')):\n",
    "                print('- %s is missing matching reconstructions' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for any .csv files in each folder.\n",
    "# These are only generated when the \"X/Y Alignment With a Reference Scan\" was performed in NRecon.\n",
    "# If those files do *not* exist we have missed to do it and should correct for this.\n",
    "Data['XYAlignment'] = [glob.glob(os.path.join(f, '*T*.csv')) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display samples which are missing the .csv-files for the XY-alignment\n",
    "for c, row in Data.iterrows():\n",
    "    # Iterate over every 'proj' folder\n",
    "    if 'proj' in row['Folder']:\n",
    "        if not row['XYAlignment']:\n",
    "            if not any(x in row.LogFile for x in ['rectmp.log',  # because we only exclude temporary logfiles in a later step\n",
    "                                                  'proj_nofilter',  # since these two scans of single teeth don't contain a reference scan\n",
    "                                                  'TScopy',  # discard *t*hermal *s*hift data\n",
    "                                                  ]):\n",
    "                print('- %s has *not* been X/Y aligned' % row.LogFile[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the logfiles from all the folders that might be on disk but that we don't want to load the data from\n",
    "for c, row in Data.iterrows():\n",
    "    if os.path.split(row.Folder)[-1] == 'proj':  # drop all projections folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'ucket' not in row.Folder:  # Remove all test scans which are not named 'Sticklbucket_*' or something else containing 'ucket'\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif '_regions' in row.Folder:  # Exclude all log files that we write in this notebook (to $scan$_region folders)\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums in the dataframe\n",
    "Data['Sample'] = [os.path.basename(log).replace('_rec.log', '') for log in Data['LogFile']]\n",
    "Data['Scan'] = [os.path.basename(os.path.dirname(log)) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quickly show the data from the last loaded scans\n",
    "Data.tail(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file names of all the reconstructions of all the scans\n",
    "Data['Filenames Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "# How many reconstructions do we have?\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data['Filenames Reconstructions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "# `find . -name \"*rec*.png\" -type f -mtime +333 -delete`\n",
    "# Based on https://stackoverflow.com/a/13851602\n",
    "# for c,row in Data.iterrows():\n",
    "#     if not row['Number of reconstructions']:\n",
    "#         print('%s contains no PNG files, we might be currently reconstructing it' % row.Folder)\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "# Reset the dataframe count/index for easier indexing afterwards\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe based on the scan date\n",
    "Data.sort_values(by=['Scan date'],\n",
    "                 ignore_index=True,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the parameters we extracted from the log files (with [our log file parser](https://github.com/habi/BrukerSkyScanLogfileRuminator)) to check for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ring removal parameters\n",
    "for machine in Data['Scanner'].unique():\n",
    "    print('For the %s we have '\n",
    "          'ringartefact-correction values of %s' % (machine,\n",
    "                                                    Data[Data.Scanner == machine]['RingartefactCorrection'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ring removal parameter\n",
    "for rac in sorted(Data['RingartefactCorrection'].unique()):\n",
    "    print('Ringartefact-correction %02s is found in %03s scans' % (rac,\n",
    "                                                                   Data[Data.RingartefactCorrection == rac]['RingartefactCorrection'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ring removal parameter for non-zero values\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('----', scanner, '----')\n",
    "    for c, row in Data[Data.Scanner == scanner].iterrows():\n",
    "        if not row.RingartefactCorrection:  # is set to 'nan' when zero, so we only show the values that are set\n",
    "            print('Fish %s scan %s was reconstructed with RAC of %s' % (row['Sample'],\n",
    "                                                                        row['Scan'],\n",
    "                                                                        row['RingartefactCorrection']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check beamhardening parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'beamhardening correction values of %s' % (scanner,\n",
    "                                                     Data[Data.Scanner == scanner]['BeamHardeningCorrection'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display beamhardening parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('----', scanner, '----')\n",
    "    for c, row in Data[Data.Scanner == scanner].iterrows():\n",
    "        if not row.BeamHardeningCorrection:  # is set to 'nan' when zero, so we only show the values that are set\n",
    "            print('Scan %s of fish %s was reconstructed with beam hardening correction of %s' % (row['Sample'],\n",
    "                                                                                                 row['Scan'],\n",
    "                                                                                                 row['BeamHardeningCorrection']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check defect pixel masking parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('For the %s we have '\n",
    "          'defect pixel masking values of %s' % (scanner,\n",
    "                                                 Data[Data.Scanner == scanner]['DefectPixelMasking'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display defect pixel masking parameters\n",
    "for dpm in sorted(Data['DefectPixelMasking'].unique()):\n",
    "    print('A defect pixel masking of %02s is found in %03s scans' % (dpm,\n",
    "                                                                     Data[Data.DefectPixelMasking == dpm]['DefectPixelMasking'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.scatterplot(data=Data, x='Fish', y='DefectPixelMasking', hue='Scanner')\n",
    "# plt.title('Defect pixel masking')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display defect pixel masking parameters\n",
    "for scanner in Data.Scanner.unique():\n",
    "    print('----', scanner, '----')\n",
    "    for c, row in Data[Data.Scanner == scanner].iterrows():\n",
    "        if row.Scanner == 'SkyScan1272' and row.DefectPixelMasking != 50:\n",
    "            print('Fish %s scan %s was reconstructed with DPM of %s' % (row['Fish'],\n",
    "                                                                        row['Scan'],\n",
    "                                                                        row['DefectPixelMasking']))\n",
    "        if row.Scanner == 'SkyScan2214' and row.DefectPixelMasking != 0:\n",
    "            print('Fish %s scan %s was reconstructed with DPM of %s' % (row['Fish'],\n",
    "                                                                        row['Scan'],\n",
    "                                                                        row['DefectPixelMasking']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now that we've double-checked some of the parameters (and corrected any issues that might have shown up) we start to load the preview images.\n",
    "If the three cells below are uncommented, the machine-generated previews are shown, otherwise we just continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['Filename PreviewImage'] = [sorted(glob.glob(os.path.join(f, '*_spr.bmp')))[0] for f in Data['Folder']]\n",
    "Data['PreviewImage'] = [dask_image.imread.imread(pip).squeeze()\n",
    "                        if pip\n",
    "                        else numpy.random.random((100, 100)) for pip in Data['Filename PreviewImage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an approximately square overview image\n",
    "lines = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)\n",
    "    plt.imshow(row.PreviewImage.squeeze())\n",
    "    plt.title(os.path.join(row['Sample'], row['Scan']))\n",
    "    plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                  'um',\n",
    "                                  color='black',\n",
    "                                  frameon=True))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Root, 'ScanOverviews.png'),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we 'load' all reconstructions from disks into stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load all reconstructions DASK arrays\n",
    "# Reconstructions = [dask_image.imread.imread(os.path.join(folder,'*rec*.png')) for folder in Data['Folder']]\n",
    "# Load all reconstructions into ephemereal DASK arrays, with a nice progress bar...\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Loading reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))\n",
    "Reconstructions = [rec[:, :, :, 0] for rec in Reconstructions]  # Get rid of the color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reconstructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do we have on disk?\n",
    "print('We have %s reconstructions on %s' % (Data['Number of reconstructions'].sum(), Root))\n",
    "print('This is about %s reconstructions per scan (%s scans in %s folders)' % (round(Data['Number of reconstructions'].sum() / len(Data)),\n",
    "                                                                              len(Data),\n",
    "                                                                              len(Data.Sample.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big are the datasets?\n",
    "Data['Size'] = [rec.shape for rec in Reconstructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Frontal',\n",
    "              'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Working on MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'], row['Scan'], direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate and save MIP\n",
    "            imageio.imwrite(outfilepath, Reconstructions[c].max(axis=d).compute().astype('uint8'))\n",
    "        Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show/save MIP slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Saving overview of MIP images',\n",
    "                   total=len(Data)):\n",
    "    outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                               '%s.%s.MIPs.png' % (row['Sample'], row['Scan']))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        for d, direction in tqdm(enumerate(directions),\n",
    "                                 desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                                 leave=False,\n",
    "                                 total=len(directions)):\n",
    "            plt.subplot(1, 3, d + 1)\n",
    "            plt.imshow(row['MIP_' + direction])\n",
    "            plt.gca().add_artist(ScaleBar(row['Voxelsize'],\n",
    "                                          'um'))\n",
    "            plt.title('%s MIP' % direction)\n",
    "            plt.axis('off')\n",
    "            plt.title('%s\\n%s MIP' % (os.path.join(row['Sample'], row['Scan']), direction))\n",
    "            plt.savefig(outfilepath,\n",
    "                        transparent=True,\n",
    "                        bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further checking the data, we look at the gray values and gray value histograms of the reconstructions.\n",
    "This helps us to find scans that have not been reconstructed well and might either need to be repeated or simply re-reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overeexposecheck(item, threshold=222, howmanypercent=0.01, whichone='Axial', verbose=False):\n",
    "    '''Function to check if a certain amount of voxels are brighter than a certain value'''\n",
    "    if (Data['MIP_%s' % whichone][item]>threshold).sum() > (Data['MIP_%s' % whichone][item].size * howmanypercent / 100):\n",
    "        if verbose:\n",
    "            plt.imshow(Data['MIP_%s' % whichone][item])\n",
    "            plt.imshow(dask.array.ma.masked_less(Data['MIP_%s' % whichone][item] > threshold, 1).compute(),\n",
    "                       cmap='viridis_r',\n",
    "                       alpha=0.5)\n",
    "            plt.title('%s/%s\\n%s px of %s Mpixels (>%s%%) are brighter '\n",
    "                      'than %s' % (Data['Sample'][item],\n",
    "                                   Data['Scan'][item],\n",
    "                                   (Data['MIP_%s' % whichone][item] > threshold).sum().compute(),\n",
    "                                   round(1e-6 * Data['MIP_%s' % whichone][item].size,2),\n",
    "                                   howmanypercent,\n",
    "                                   threshold))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][item],\n",
    "                                          'um'))\n",
    "            plt.show()\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'too much' of the MIP is overexposed\n",
    "# TODO: How much is too much?\n",
    "Data['OverExposed'] = [overeexposecheck(c,\n",
    "                                        whichone='Frontal',\n",
    "                                        verbose=True) for c, row in Data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of all the images\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "subsample = 1\n",
    "Data['Histogram'] = [dask.array.histogram(rec[::subsample, ::subsample, ::subsample],\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for rec in Reconstructions]\n",
    "# Actually compute the data and put only h into the dataframe, so that we can easily plot them below\n",
    "# Discard the bins\n",
    "Data['Histogram'] = [h.compute() for h, b in Data['Histogram']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all single histograms\n",
    "for c, row in Data.iterrows():\n",
    "    if subsample > 1:\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Histogram.%02dx_subsampled.png' % (row['Sample'],\n",
    "                                                                             row['Scan'],\n",
    "                                                                             subsample))\n",
    "    else:\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.Histogram.png' % (row['Sample'],\n",
    "                                                            row['Scan']))\n",
    "    if not os.path.exists(outfilepath):\n",
    "        plt.subplot(121)\n",
    "        plt.plot(row.Histogram)\n",
    "        plt.title('Linear')\n",
    "        plt.xlim([0, 2**8])\n",
    "        plt.ylim(ymin=0)\n",
    "        seaborn.despine()\n",
    "        plt.subplot(122)\n",
    "        plt.semilogy(row.Histogram)\n",
    "        plt.title('Logarithmic')\n",
    "        plt.xlim([0, 2**8])\n",
    "        plt.ylim(ymin=10**0)\n",
    "        seaborn.despine()\n",
    "        if subsample > 1:\n",
    "            plt.suptitle('Histogram of all %s reconstructions (%sx subsampled) of %s' % (row['Number of reconstructions'],\n",
    "                                                                                         subsample,\n",
    "                                                                                         row['Sample']))\n",
    "        else:\n",
    "            plt.suptitle('Histogram of all %s reconstructions of %s' % (row['Number of reconstructions'], row['Sample']))\n",
    "        plt.savefig(outfilepath,\n",
    "                    transparent=True,\n",
    "                    bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all histograms together\n",
    "for c, row in Data.iterrows():\n",
    "    plt.semilogy(row.Histogram, label=row['Sample'])\n",
    "    plt.xlim([0, 2**8])\n",
    "    plt.ylim(ymin=10**0)\n",
    "    seaborn.despine()\n",
    "    if subsample > 1:\n",
    "        plt.title('Histogram of all %s scans (%sx subsampled)' % (len(Data), subsample))\n",
    "    else:\n",
    "        plt.title('Histogram of all %s scans' % (len(Data)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have previewed %s scans of %s folders with reconstructions in %s' % (len(Data),\n",
    "                                                                               len(Data.Sample.unique()),\n",
    "                                                                               Root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
