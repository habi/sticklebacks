{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the scans\n",
    "\n",
    "We scanned multiple fish in [a special sample holder](https://github.com/TomoGraphics/Hol3Drs/blob/master/STL/Stickleback.Multiple.stl).\n",
    "This notebook is used to separate them into different bunch of reonstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are used to set up the whole notebook.\n",
    "They load needed libraries and set some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import nrrd\n",
    "import numpy\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "# This is loaded as a submodule to alleviate excessive copy-pasting between *all* projects we do\n",
    "# See https://github.com/habi/BrukerSkyScanLogfileRuminator for details on its inner workings\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code linting\n",
    "# %load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "# We use the fast internal SSD for speed reasons\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "# plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the (tomographic) data can reside on different drives we set a folder to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        BasePath = os.path.join('N:\\\\')\n",
    "Root = os.path.join(BasePath, 'IEE Stickleback')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up, actually start to load/ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Using os.walk is way faster than using recursive glob.glob, see DataWrangling.ipynb for details\n",
    "# Not sorting the found logfiles is also making it quicker\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a (small) sampler of the loaded data as a first check\n",
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the logfiles from all the folders that might be on disk but that we don't want to load the data from\n",
    "for c, row in Data.iterrows():\n",
    "    # Since this notebook only deals with the 'BucketOfFish' scans, drop all others\n",
    "    if 'ucket' not in row.Folder:  # Only use the scans named Bucket* here\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rec' not in row.Folder:  # Only look at logs in the rec folders\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif '_regions' in row.Folder:  # Exclude all log files that we write in this notebook (to $scan$_region folders)\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:  # Exclude any log files from rsyncing temporary data\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:  # Exclude any log files from rsyncing temporary data\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums in the dataframe\n",
    "Data['Sample'] = [os.path.basename(logfile).replace('_rec.log', '') for logfile in Data['LogFile']]\n",
    "Data['Scan'] = [os.path.basename(os.path.dirname(logfile)) for logfile in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does the dataframe look plausible?\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file names of all the reconstructions of all the scans\n",
    "Data['Filenames Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "# How many reconstructions do we have?\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data['Filenames Reconstructions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "# Reset the dataframe count/index for easier indexing afterwards\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe based on the scan date\n",
    "Data.sort_values(by=['Scan date'],\n",
    "                 ignore_index=True,\n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the dataframe look plausible?\n",
    "Data[['Sample', 'Scan', 'Scan date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load all reconstructions DASK arrays\n",
    "# Reconstructions = [dask_image.imread.imread(os.path.join(folder,'*rec*.png')) for folder in Data['Folder']]\n",
    "# Load all reconstructions into ephemereal DASK arrays, with a nice progress bar...\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Loading reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))\n",
    "Reconstructions = [rec[:, :, :, 0] for rec in Reconstructions]  # Get rid of the color channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bucket name\n",
    "Data['Bucket'] = [(log).split(os.sep)[-3].split('_')[-1] for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "directions = ['Axial',\n",
    "              'Frontal',\n",
    "              'Median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Working on MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'], row['Scan'], direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate and write out MIP\n",
    "            imageio.imwrite(outfilepath, Reconstructions[c].max(axis=d).compute())\n",
    "        Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    # Based on https://stackoverflow.com/a/55110923\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert labels.max() != 0  # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:]) + 1\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vial_label_extractor(whichscan, threshold=35, part=333, verbose=True):\n",
    "    bottom_mip_filename = os.path.join(os.path.dirname(Data['Folder'][whichscan]),\n",
    "                                       '%s.%s.MIP.Bottom%04dslices.png' % (Data['Sample'][whichscan], Data['Scan'][whichscan], part))\n",
    "    # Generate and write out file to speed up process\n",
    "    if not os.path.exists(bottom_mip_filename):\n",
    "        # Let's get out the numbers, they are 'hidden' in the lower part\n",
    "        bottom_mip = Reconstructions[whichscan][:part].max(axis=0)\n",
    "        imageio.imwrite(bottom_mip_filename, bottom_mip)\n",
    "    bottom_mip = dask_image.imread.imread(bottom_mip_filename).squeeze().compute()\n",
    "    # Clean central part\n",
    "    region_radius = 100\n",
    "    bottom_mip[bottom_mip.shape[0] // 2 - region_radius:bottom_mip.shape[0] // 2 + region_radius,\n",
    "               bottom_mip.shape[1] // 2 - region_radius:bottom_mip.shape[1] // 2 + region_radius] = 0\n",
    "    if not threshold:\n",
    "        # Calculate multi Otsu with three classes, use highest threshold\n",
    "        threshold = skimage.filters.threshold_multiotsu(bottom_mip)[-1]\n",
    "        # For at least bucket E, the threshold is borked\n",
    "        # If it's larger than 50, set it to something reasonable\n",
    "        if threshold > 50:\n",
    "            threshold = 35\n",
    "    # remove largest component from thresholded bottom MIP\n",
    "    # The largest component are the separation walls of the bucket\n",
    "    numbers = numpy.bitwise_xor(bottom_mip > threshold, getLargestCC(bottom_mip > threshold))\n",
    "    # Clean up the image by removing small objects\n",
    "    numbers_cleaned = skimage.morphology.remove_small_objects(numbers, min_size=10000)\n",
    "    # only labels should remain\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(Data['MIP_Frontal'][whichscan],\n",
    "                   vmin=0,\n",
    "                   vmax=2**8)\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.gca().add_artist(matplotlib.patches.Rectangle((0, 0), Data['MIP_Frontal'][whichscan].shape[1],\n",
    "                                                          part,\n",
    "                                                          edgecolor=None,\n",
    "                                                          facecolor='yellow',\n",
    "                                                          alpha=0.618))\n",
    "        plt.title('Bucket %s' % Data['Bucket'][whichscan])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(Data['MIP_Axial'][whichscan],\n",
    "                   vmin=0,\n",
    "                   vmax=2**8)\n",
    "        plt.imshow(numpy.ma.masked_equal(numbers_cleaned, 0), cmap='viridis_r', alpha=0.618)\n",
    "        plt.title('MIP of marked region\\n%s recs>%s - their largest CC' % (part, threshold))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.savefig('%s.%s.Labels.Overview.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "        plt.show()\n",
    "    return numbers_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vial_label_extractor(4, verbose=True, threshold=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the extractor thingamajig\n",
    "vial_label_extractor(1, verbose=True, part=400, threshold=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['VialLabels'] = [vial_label_extractor(i, verbose=False, threshold=None) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fish_position(whichscan, threshold=None, verbose=False):\n",
    "    import matplotlib.patches\n",
    "    # Detect the fish positions based on blobs in the top-down MIP\n",
    "    regions = None\n",
    "    td_mip = Data['MIP_Axial'][whichscan].compute()\n",
    "    if not threshold:\n",
    "        threshold = skimage.filters.threshold_otsu(td_mip[td_mip > 10])\n",
    "    td_mip_thresholded = td_mip > threshold\n",
    "    # Remove central part, on some scans the connector shows up...\n",
    "    region_radius = 200\n",
    "    td_mip_thresholded[td_mip_thresholded.shape[0] // 2 - region_radius:td_mip_thresholded.shape[0] // 2 + region_radius,\n",
    "                       td_mip_thresholded.shape[1] // 2 - region_radius:td_mip_thresholded.shape[1] // 2 + region_radius] = 0\n",
    "    # Clean speckles, assuming all fish are larger than 5000 px\n",
    "    cleaned = skimage.morphology.remove_small_objects(td_mip_thresholded,\n",
    "                                                      min_size=5000)\n",
    "    # Remove central part, on some scans the connector shows up...\n",
    "    region_radius = 275\n",
    "    cleaned[cleaned.shape[0] // 2 - region_radius:cleaned.shape[0] // 2 + region_radius, cleaned.shape[1] // 2 - region_radius:cleaned.shape[1] // 2 + region_radius] = 0\n",
    "    # Label image\n",
    "    label_image = skimage.measure.label(cleaned)\n",
    "    # Detect regions\n",
    "    regions = skimage.measure.regionprops(label_image)\n",
    "    # Drop small areas, if we found more than 6 fish\n",
    "    if len(regions) > 6:\n",
    "        print('Found more than 6 regions')\n",
    "        print('Dropping region with area < 1000 from %s found regions' % len(regions))\n",
    "        regions = [item for item in regions if item.area > 1000]\n",
    "    if len(regions) < 6:\n",
    "        print('Found less than 6 regions')\n",
    "        # regions = (regions + 6 * [numpy.nan])[:6]\n",
    "        # print(regions)\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(td_mip)\n",
    "        plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], 0), cmap='viridis_r', alpha=0.618)\n",
    "        plt.title('Bucket %s' % Data['Bucket'][whichscan])\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122)\n",
    "        # to make the background transparent, pass the value of `bg_label`,\n",
    "        # and leave `bg_color` as `None` and `kind` as `overlay`\n",
    "        plt.imshow(skimage.color.label2rgb(label_image, image=td_mip, bg_label=0))\n",
    "        for c, region in enumerate(regions):\n",
    "            if region.bbox:\n",
    "                # draw rectangle around segmented fish\n",
    "                minr, minc, maxr, maxc = region.bbox\n",
    "                rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                                    fill=False, edgecolor='white', ls='--')\n",
    "                plt.gca().add_patch(rect)                \n",
    "                plt.scatter(region.centroid[1], region.centroid[0], s=200, c='black', edgecolors='white')\n",
    "                plt.annotate('%s' % region.label,\n",
    "                             xy=(region.centroid[1], region.centroid[0]),\n",
    "                             ha='center',\n",
    "                             va='center',\n",
    "                             color='white')\n",
    "        plt.title('%s detected fish' % len(regions))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.savefig('%s.%s.Labels.Detected.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "        plt.show()\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test position detector\n",
    "test = detect_fish_position(0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_list(inputlist, neworder=[0, 1, 3, 5, 4, 2], verbose=True):\n",
    "    \"\"\"\n",
    "    Shuffle fish positions to a new order.\n",
    "    The default neworder is the \"usual\" order we find the fish in.\n",
    "    We *deliberately* want a new list, so we can keep the old one around for double-checks\n",
    "    https://stackoverflow.com/questions/2177590/how-can-i-reorder-a-list#comment106984501_2177607\n",
    "    Since double-checking is cumbersome, we print out both lists.\n",
    "    \"\"\"\n",
    "    # Catch less than 6 fish in bucket\n",
    "    if len(inputlist) != len(neworder):\n",
    "        print('We found less than six fish, so we simply return the original, unsorted list.')\n",
    "        print('Call the \"reorder_list\" command with (for example) \"neworder=%s\".' % random.sample(range(len(inputlist)), len(inputlist)))\n",
    "        return inputlist\n",
    "    else:\n",
    "        ordered_list = [inputlist[i] for i in neworder]\n",
    "        if verbose:\n",
    "            print('Clockwise Original : %s' % [element.label for element in inputlist])\n",
    "            print('Clockwise Reordered: %s' % [element.label for element in ordered_list])\n",
    "    return ordered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the fish positions and sort them in a default order in a first pass\n",
    "Data['Regions'] = [detect_fish_position(i, verbose=False) for i in range(len(Data))]\n",
    "Data['Regions_Ordered'] = [reorder_list(regions, verbose=False) for regions in Data['Regions']]\n",
    "# Below we look at each of the scans again and reorder if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some fish regions are not detected in a consistent order, reorder them correctly now.\n",
    "# # The `verbose` output makes it easy to double-check the correct labels.\n",
    "# # Bucket C: label 4 and 5 are swapped\n",
    "# whichbucket = 2\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket D: label 4 and 5 are swapped\n",
    "# whichbucket = 3\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket E: label 4 and 5 are swapped\n",
    "# whichbucket = 4\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket F: we found 7 things to label :)\n",
    "# whichbucket = 5\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 3, 5, 4, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket G\n",
    "# whichbucket = 6\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket H\n",
    "# whichbucket = 7\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 3, 5, 4, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket I\n",
    "# whichbucket = 8\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket J\n",
    "# whichbucket = 9\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket K\n",
    "# whichbucket = 10\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 3, 5, 4, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket M\n",
    "# whichbucket = 12\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket N\n",
    "# whichbucket = 13\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket O\n",
    "# whichbucket = 14\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket Q\n",
    "# whichbucket = 16\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket R\n",
    "# whichbucket = 17\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 3, 5, 4, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket S\n",
    "# whichbucket = 18\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 3, 5, 4, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket T\n",
    "# whichbucket = 19\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 4, 5, 3, 2],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket U\n",
    "# whichbucket = 20\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     neworder=[0, 1, 2, 3],  # Only 4 fish!\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions from the second (Sticklebucket) batch\n",
    "whichbucket = 2\n",
    "print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "_ = detect_fish_position(whichbucket, verbose=True)\n",
    "Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "                                                    neworder=[0, 1, 2, 4, 3],\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions from the second (Sticklebucket) batch\n",
    "whichbucket = 3\n",
    "print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "_ = detect_fish_position(whichbucket, verbose=True)\n",
    "Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "                                                    neworder=[0, 1, 4, 5, 3, 2],\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions from the second (Sticklebucket) batch\n",
    "whichbucket = 4\n",
    "print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "_ = detect_fish_position(whichbucket, verbose=True)\n",
    "Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "                                                    neworder=[0, 1, 4, 5, 3, 2],\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions from the second (Sticklebucket) batch\n",
    "whichbucket = 5\n",
    "print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "_ = detect_fish_position(whichbucket, verbose=True)\n",
    "Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "                                                    neworder=[0, 2, 3, 4, 1],\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions from the second (Sticklebucket) batch\n",
    "whichbucket = 7\n",
    "print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "_ = detect_fish_position(whichbucket, verbose=True)\n",
    "Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "                                                    neworder=[0, 2, 3, 5, 4, 1],\n",
    "                                                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reorder regions from the second (Sticklebucket) batch\n",
    "# whichbucket = 8\n",
    "# print('Looking at Bucket %s' % Data['Bucket'][whichbucket])\n",
    "# _ = detect_fish_position(whichbucket, verbose=True)\n",
    "# Data['Regions_Ordered'][whichbucket] = reorder_list(Data['Regions'][whichbucket],\n",
    "#                                                     # neworder=[0, 2, 3, 5, 4, 1],\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEAT THE ABOVE FOR EACH NEWLY ACQUIRED BUCKET\n",
    "# WITH VERBOSE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct us a *consecutive* fish number, based on 6 fish per scan\n",
    "# # This 'FishNumber' is *not* the same as the 'Fish Number' in Bens tracking sheet\n",
    "# # We use our FishNumber to construct a FishID, which matches the 'Fish Number'\n",
    "# Data['FishNumber'] = [[reg.label + (6 * c - 1) for reg in region] for c, region in enumerate(Data.Regions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Overwrite blunders in first two batches with their correct numbers\n",
    "# # Bucket A\n",
    "# Data['FishNumber'][0] = [fn + 1 for fn in range(len(Data['FishNumber'][0]))]\n",
    "# Data['FishNumber'][0][2] = 30\n",
    "# # Bucket B\n",
    "# Data['FishNumber'][1][0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in tracking sheet from Ben\n",
    "# trackingsheet = pandas.read_excel('/home/habi/research_storage_ben/microCT_Stickleback/CT_Sticklebacks_Tracking_Sheet.xlsx', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackingsheet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackingsheet.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackingsheet['Unique_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct the fish IDs\n",
    "# # This ID corresponds to the 'Unique_ID' in the tracking sheet read in above\n",
    "# # Set the ID for all buckets, technically they are only valid for buckets A-D\n",
    "# Data['FishID'] = [['FG.X23.%03d' % n for n in number] for number in Data.FishNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update fish ID for later scans\n",
    "# # In bucket E we start with the SL fish\n",
    "# Data['FishID'][5] = ['SL.X23.%03d' % int(c + 1) for c, id in enumerate(Data['FishID'][5])]\n",
    "# # The first fish of Bucket E is still an FG one\n",
    "# Data['FishID'][5][-1] = 'FG.X23.031'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Buckets F-J are SL fish\n",
    "# Data['FishID'][6] = ['SL.X23.%03d' % int(c + 1 + 5) for c, id in enumerate(Data['FishID'][6])]\n",
    "# Data['FishID'][7] = ['SL.X23.%03d' % int(c + 1 + 5 + 6) for c, id in enumerate(Data['FishID'][7])]\n",
    "# Data['FishID'][8] = ['SL.X23.%03d' % int(c + 1 + 5 + 6 + 6) for c, id in enumerate(Data['FishID'][8])]\n",
    "# Data['FishID'][9] = ['SL.X23.%03d' % int(c + 1 + 5 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][9])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In bucket J we start with the SR fish\n",
    "# Data['FishID'][9][3] = 'SR.X23.001'\n",
    "# Data['FishID'][9][4] = 'SR.X23.002'\n",
    "# Data['FishID'][9][5] = 'SR.X23.003'\n",
    "# # Bucket K-P are SR fish\n",
    "# Data['FishID'][10] = ['SR.X23.%03d' % int(c + 1 + 3) for c, id in enumerate(Data['FishID'][10])]\n",
    "# Data['FishID'][11] = ['SR.X23.%03d' % int(c + 1 + 3 + 6) for c, id in enumerate(Data['FishID'][11])]\n",
    "# Data['FishID'][12] = ['SR.X23.%03d' % int(c + 1 + 3 + 6 + 6) for c, id in enumerate(Data['FishID'][12])]\n",
    "# Data['FishID'][13] = ['SR.X23.%03d' % int(c + 1 + 3 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][13])]\n",
    "# Data['FishID'][14] = ['SR.X23.%03d' % int(c + 1 + 3 + 6 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][14])]\n",
    "# Data['FishID'][15] = ['SR.X23.%03d' % int(c + 1 + 3 + 6 + 6 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][15])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In bucket P we start with the WT fish\n",
    "# Data['FishID'][15][2] = 'WT.X23.001'\n",
    "# Data['FishID'][15][3] = 'WT.X23.002'\n",
    "# Data['FishID'][15][4] = 'WT.X23.003'\n",
    "# Data['FishID'][15][5] = 'WT.X23.004'\n",
    "# # Bucket Q- are WR fish\n",
    "# Data['FishID'][16] = ['WR.X23.%03d' % int(c + 1 + 4) for c, id in enumerate(Data['FishID'][16])]\n",
    "# Data['FishID'][17] = ['WR.X23.%03d' % int(c + 1 + 4 + 6) for c, id in enumerate(Data['FishID'][17])]\n",
    "# Data['FishID'][18] = ['WR.X23.%03d' % int(c + 1 + 4 + 6 + 6) for c, id in enumerate(Data['FishID'][18])]\n",
    "# Data['FishID'][19] = ['WR.X23.%03d' % int(c + 1 + 4 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][19])]\n",
    "# Data['FishID'][20] = ['WR.X23.%03d' % int(c + 1 + 4 + 6 + 6 + 6 + 6) for c, id in enumerate(Data['FishID'][20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(whichscan, verbose=True):\n",
    "    \"\"\"\n",
    "    For the first batch we constructed a consecutive FishID.\n",
    "    For the second (Sticklebucket) batch, we read a file on disk to map the fish positions to the labels\n",
    "    \"\"\"\n",
    "    # Find the file we want to use for mappingdef reorder_list(list, neworder=[0, 2, 4, 5, 3, 1], verbose=True):\n",
    "    mapping_file = glob.glob(os.path.join(os.path.dirname(Data['Folder'][whichscan]), '*Mapping*.md'))\n",
    "    if not len(mapping_file):\n",
    "        print('You will need to provide a mapping file for this scan')\n",
    "        return\n",
    "    # The mapping file is a markdown table with 'FishID', 'Petal' (sample holder compartment), and 'Region' (detected fish position)\n",
    "    # We want to use this file to do the mapping, eliminating manual guesswork and reordering :)\n",
    "    # Read the file into a dataframe\n",
    "    # We made a little markdown-formatted list; if we use | as a separator, then we can easily get the file into a dataframe :)\n",
    "    # Unfortunately, this leads to spaces in the column names, so we strip those afterwards   \n",
    "    mapping_df = pandas.read_csv(mapping_file[0], sep='|')\n",
    "    mapping_df = mapping_df.replace({' None ': None})  # Replace ' None ' with None\n",
    "    mapping_df.columns = mapping_df.columns.str.replace(' ', '')  # Strip spaces from column names\n",
    "    # Drop the first line, which is the table separator\n",
    "    mapping_df.drop(0, inplace=True)\n",
    "    # Drop the 'Unnamed' columns, which are empty: https://stackoverflow.com/a/52696683/323100\n",
    "    mapping_df.dropna(how='all', axis='columns', inplace=True)\n",
    "    # Check if a fish has *not* been scanned, which we noted as 'None' in the .md file\n",
    "    # Simply drop this row and reset the index\n",
    "    # Iterate over rows and handle 'Petal' values\n",
    "    for c, row in mapping_df.iterrows():\n",
    "        try:\n",
    "            if row['Petal'] is not None and int(row['Petal']):  # Check if 'Petal' is not None and can be converted to int\n",
    "                pass\n",
    "        except ValueError:\n",
    "            if verbose:\n",
    "                print('Dropping \"%s\" for %s' % (row['Petal'], row['Fish']))\n",
    "            mapping_df.drop([c], inplace=True)\n",
    "    mapping_df = mapping_df.reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print(mapping_df)\n",
    "    # FishIDs = [id for id in mapping_df.Fish]\n",
    "    return [id.strip() for id in mapping_df.Fish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mapping(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['FishID'] = [mapping(i) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['FishID'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[['Bucket', 'FishID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load overview and labbook image, if present\n",
    "Data['LabbookImage'] = [dask_image.imread.imread(os.path.join(os.path.dirname(f), '_labbook.jpg')).squeeze()\n",
    "                        if os.path.exists(os.path.join(os.path.dirname(f), '_labbook.jpg'))\n",
    "                        else numpy.random.random((2**6, 2**6))\n",
    "                        for f in Data['Folder']]\n",
    "Data['OverviewImage'] = [dask_image.imread.imread(os.path.join(os.path.dirname(f), '_overview.jpg')).squeeze()\n",
    "                         if os.path.exists(os.path.join(os.path.dirname(f), '_overview.jpg'))\n",
    "                         else numpy.random.random((2**6, 2**6))\n",
    "                         for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublecheck_fish_position(whichscan):\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(Data['MIP_Axial'][whichscan])\n",
    "    plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], False), cmap='viridis_r')\n",
    "    for c, region in enumerate(Data['Regions'][whichscan]):\n",
    "        plt.annotate('%s' % str(c + 1),\n",
    "                     xy=(region.centroid[1] + 222, region.centroid[0]),\n",
    "                     color='black',\n",
    "                     va='center',\n",
    "                     bbox=dict(fc=\"white\", alpha=0.618))\n",
    "    plt.title('MIP & Calculated labels')\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(Data['MIP_Axial'][whichscan])\n",
    "    plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], False), cmap='viridis_r')\n",
    "    for c, region in enumerate(Data['Regions_Ordered'][whichscan]):\n",
    "        plt.annotate('%s:%s' % (c + 1, Data['FishID'][whichscan][c]),\n",
    "                     xy=(region.centroid[1], region.centroid[0]),\n",
    "                     color='black',\n",
    "                     fontsize=8,\n",
    "                     va='center',\n",
    "                     ha='center',\n",
    "                     bbox=dict(fc=\"white\", alpha=0.618))\n",
    "        # draw rectangle around segmented fish\n",
    "        # Bounding box (min_row, min_col, max_row, max_col)\n",
    "        min_row, min_col, max_row, max_col = region.bbox\n",
    "        bx = (min_col - buffer, max_col + buffer, max_col + buffer, min_col - buffer, min_col - buffer)\n",
    "        by = (min_row - buffer, min_row - buffer, max_row + buffer, max_row + buffer, min_row + buffer)\n",
    "        plt.plot(bx, by, '--')\n",
    "        # minr, minc, maxr, maxc = region.bbox\n",
    "        # bx = (minc, maxc, maxc, minc, minc)\n",
    "        # by = (minr, minr, maxr, maxr, minr)\n",
    "        # plt.plot(bx, by, '-r')\n",
    "        # plt.scatter(region.centroid[1], region.centroid[0])\n",
    "    plt.title('Resorted label:mapped ID')\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(Data['LabbookImage'][whichscan])\n",
    "    plt.axis('off')\n",
    "    plt.title('Photo of labbook')\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(Data['OverviewImage'][whichscan])\n",
    "    plt.axis('off')\n",
    "    plt.title('Photo of tubes')\n",
    "    plt.suptitle('Bucket %s' % Data['Bucket'][whichscan])\n",
    "    plt.savefig('%s.%s.Labels.Check.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping(5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test checking function\n",
    "buffer = 50\n",
    "doublecheck_fish_position(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 50\n",
    "for i in range(len(Data)):\n",
    "    doublecheck_fish_position(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regionextractor(whichscan, buffer=50, verbose=True):\n",
    "    os.makedirs(Data.Folder[whichscan] + '_regions', exist_ok=True)\n",
    "    for c, region in tqdm(enumerate(Data['Regions_Ordered'][whichscan]),\n",
    "                          total=len(Data['Regions_Ordered'][whichscan]),\n",
    "                          desc='Extracting and visualizing regions'):\n",
    "        outputfilename = os.path.join(Data.Folder[whichscan] + '_regions', 'region_%s_%s.zarr' % (str(c + 1),\n",
    "                                                                                                  Data['FishID'][whichscan][c]))\n",
    "        if not os.path.exists(outputfilename):\n",
    "            # Crop current region out of reconstructions stack, drop RGB axis and rechunk, making for even more efficient access\n",
    "            currentregion = Reconstructions[whichscan][:, region.bbox[0] - buffer:region.bbox[2] + buffer, region.bbox[1] - buffer:region.bbox[3] + buffer].rechunk('auto')\n",
    "            if verbose:\n",
    "                print('Writing to %s. This takes a while...' % outputfilename[len(Root) + 1:])\n",
    "            dask.array.to_zarr(currentregion, outputfilename)\n",
    "        if verbose:\n",
    "            # Read written file back in, so we can profit from the rechunking\n",
    "            currentregion = dask.array.from_zarr(outputfilename)\n",
    "            plt.subplot(2, 6, c + 1)\n",
    "            plt.imshow(Data['MIP_Axial'][whichscan][region.bbox[0] - buffer:region.bbox[2] + buffer, region.bbox[1] - buffer:region.bbox[3] + buffer])\n",
    "            plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan][region.bbox[0] - buffer:region.bbox[2] + buffer, region.bbox[1] - buffer:region.bbox[3] + buffer],\n",
    "                                             False),\n",
    "                       cmap='viridis_r')\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "            plt.title('Cut of MIP\\n%s' % str(c + 1))\n",
    "            plt.axis('off')\n",
    "            # plt.subplot(6, 2, (2 * c ) + 2)\n",
    "            plt.subplot(2, 6, c + 1 + 6)\n",
    "            # Recalculate MIP for double-checking\n",
    "            plt.imshow(currentregion.max(axis=0))\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "            plt.title('MIP of cut\\n%s' % Data['FishID'][whichscan][c])\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('Bucket %s' % Data['Bucket'][whichscan])\n",
    "    if verbose:\n",
    "        plt.savefig('%s.%s.Regions.Check.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan] + '_regions'),\n",
    "                                                              Data.Sample[whichscan]),\n",
    "                                                 Data.Scan[whichscan]))\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 50\n",
    "regionextractor(0, buffer=buffer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trackingsheet.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly state the buffer, we want it later for adding the crop region to the regional log files\n",
    "buffer = 50\n",
    "for i in range(len(Data)):\n",
    "    regionextractor(i, buffer=buffer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doubleckeck our labels with the tracking sheet\n",
    "# for c, row in Data.iterrows():\n",
    "#     print(20 * '-', row.Bucket, 20 * '-')\n",
    "#     print('FishNumber |', 'Bens ID |', 'Our ID')\n",
    "#     for d, fishnumber in enumerate(row.FishNumber):\n",
    "#         print(fishnumber,\n",
    "#               trackingsheet.iloc[fishnumber - 1]['Unique_ID'],\n",
    "#               row.FishID[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in regions again\n",
    "# Remember to sort them here, otherwise we'll mix them up willy-nilly\n",
    "RegionZarrFiles = [sorted(glob.glob(os.path.join(folder + '_regions', '*.zarr'))) for folder in Data['Folder']]\n",
    "Regions = [[dask.array.from_zarr(f) for f in files] for files in RegionZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to generate log files for the cutout regions\n",
    "# Aeons ago, we wrote a little wrapper function to log stuff at TOMCAT\n",
    "# https://github.com/habi/TOMCAT/blob/master/postscan/StackedScanOverlapFinder.py#L104\n",
    "# The function below is slightly tweaked from there\n",
    "def myLogger(logfilename, verbose=False):\n",
    "    import logging\n",
    "    logger = logging.getLogger(logfilename)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.FileHandler(logfilename, 'w')\n",
    "    logger.addHandler(handler)\n",
    "    if verbose:\n",
    "        print('Logging to %s' % logfilename)\n",
    "    return logger\n",
    "# Then write to the file with\n",
    "# logfile = myLogger(Filename))\n",
    "# logfile.info('Put this into the log file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out a log file\n",
    "for c, row in tqdm(Data.iterrows(), total=len(Data), desc='Writing log files for regions'):\n",
    "    for d, region in tqdm(enumerate(row.Regions),\n",
    "                          total=len(row.Regions),\n",
    "                          desc=Data.Folder[c][len(Root) + 1:],\n",
    "                          leave=False):\n",
    "        # Generate output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions', row['FishID'][d])\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        # Generate logfile name\n",
    "        logfilename = os.path.join(outputdir, row['FishID'][d] + '.log')\n",
    "        # Delete logfile, if it already exists\n",
    "        if os.path.exists(logfilename):\n",
    "            os.remove(logfilename)\n",
    "        logfile = myLogger(logfilename)\n",
    "        logfile.info('Scan = %s' % os.path.join(row.Sample, row.Scan))\n",
    "        logfile.info('Voxel size = %s um' % row.Voxelsize)\n",
    "        logfile.info('ID = %s' % row['FishID'][d])\n",
    "        logfile.info('Vial = %s' % str(d + 1))\n",
    "        logfile.info('Centroid (x,y) in the original stack = %s, %s' % (int(round(region.centroid[1])), int(round(region.centroid[0]))))\n",
    "        logfile.info('Bounding box (x1:x2, y1:y2) of this region in the original stack = %s:%s, %s:%s' % (region.bbox[1] - buffer, region.bbox[3] + buffer,\n",
    "                                                                                                          region.bbox[0] - buffer, region.bbox[2] + buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out one .nrrd file per extracted region\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out .nrrd file for each region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(Regions[c]),\n",
    "                            total=len(Regions[c]),\n",
    "                            desc=Data.Folder[c][len(Root) + 1:],\n",
    "                            leave=False):\n",
    "        # Generate output name\n",
    "        outputname = os.path.join(row.Folder + '_regions', row['FishID'][d] + '.nrrd')\n",
    "        # Generate header that goes into the file\n",
    "        # https://pynrrd.readthedocs.io/en/stable/examples.html#example-with-fields-and-custom-fields\n",
    "        header = {'encoding': 'raw',\n",
    "                  'units': ['mm', 'mm', 'mm'],\n",
    "                  'spacings': [row.Voxelsize / 1000, row.Voxelsize / 1000, row.Voxelsize / 1000]}\n",
    "        # Write out file with https://github.com/mhe/pynrrd/\n",
    "        if not os.path.exists(outputname):\n",
    "            nrrd.write(outputname,\n",
    "                       zarrfile.compute(),\n",
    "                       header,\n",
    "                       index_order='C'  # with c-index order we get the data out with the stack in z-direction\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imsaver(image, filename):\n",
    "    '''\n",
    "    Function for parallelizing writing out images\n",
    "    '''\n",
    "    if not os.path.exists(filename):  # only do something if there's no image on disk yet\n",
    "        # if image.mean():  # only write something if there's something in the image\n",
    "        imageio.imwrite(filename, image.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out PNG slices\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out PNGs for each region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(Regions[c]),\n",
    "                            total=len(Regions[c]),\n",
    "                            desc=Data.Folder[c][len(Root) + 1:],\n",
    "                            leave=False):\n",
    "        # print(zarrfile.shape)\n",
    "        # plt.imshow(zarrfile[666])\n",
    "        # plt.show()\n",
    "        # Make output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions', row['FishID'][d])\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        outputfilenames = [os.path.join(outputdir,\n",
    "                                        os.path.basename(fn).replace(Data.Sample[c], Data['FishID'][c][d])) for fn in Data['Filenames Reconstructions'][c]]\n",
    "        parallelize = True\n",
    "        if parallelize:\n",
    "            # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing\n",
    "            # It is paramount that the filenames are sorted though!\n",
    "            Parallel(n_jobs=-1)(delayed(imsaver)(zarrfile[slice],\n",
    "                                                 outputfilenames[slice]) for slice in range(len(outputfilenames)))\n",
    "        else:\n",
    "            for slice in tqdm(range(len(outputfilenames)),\n",
    "                              total=len(outputfilenames),\n",
    "                              desc='%s' % os.path.splitext(RegionZarrFiles[c][d])[0][len(Root) + 1:],\n",
    "                              leave=False):\n",
    "                if not os.path.exists(outputfilenames[slice]):\n",
    "                    imageio.imwrite(outputfilenames[slice], zarrfile[slice].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholder(stack, discard=5, verbose=False):\n",
    "    '''\n",
    "    Threshold function to reliably threshold *only* bones\n",
    "    A simple 4-class multiotsu, returning only the middle threshold\n",
    "    '''\n",
    "    thresholds = skimage.filters.threshold_multiotsu(stack[stack > discard].compute(),\n",
    "                                                     classes=4)\n",
    "    if verbose:\n",
    "        histogram, bins = dask.array.histogram(stack,\n",
    "                                               bins=2**8,\n",
    "                                               range=[0, 2**8])\n",
    "        plt.semilogy(histogram)\n",
    "        plt.axvline(discard,\n",
    "                    label='completely discarded, below %s' % discard,\n",
    "                    color='red')\n",
    "        for t in thresholds:\n",
    "            plt.axvline(t, label='threshold %s' % t)\n",
    "        plt.xlim([0, 2**8])\n",
    "        plt.legend()\n",
    "        seaborn.despine()\n",
    "        plt.show()\n",
    "    # Return only the middle threshold value\n",
    "    return thresholds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichbucket = 4\n",
    "whichID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = thresholder(Regions[whichbucket][whichID], verbose=True)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 1500\n",
    "plt.subplot(131)\n",
    "plt.imshow(Regions[whichbucket][whichID][slice])\n",
    "plt.title('%s' % Data.FishID[whichbucket][whichID])\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.imshow((Regions[whichbucket][whichID][slice] > threshold))\n",
    "plt.title('%s > %s' % (Data.FishID[whichbucket][whichID], threshold))\n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(Regions[whichbucket][whichID][slice])\n",
    "plt.imshow(dask.array.ma.masked_equal((Regions[whichbucket][whichID][slice] > threshold), 0).compute(),\n",
    "           cmap='viridis_r',\n",
    "           alpha=0.618)\n",
    "plt.title('Overlay')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate threshold for each separated region\n",
    "Data['RegionThreshold'] = [[thresholder(rg) for rg in regions] for regions in Regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['RegionThreshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c, row in Data.iterrows():\n",
    "#     print(row.Bucket)\n",
    "#     for d, region in enumerate(Regions[c]):\n",
    "#         plt.imshow(region[len(region) // 5] > row['RegionThreshold'][d] * 2)\n",
    "#         plt.title('Slice %s of %s > %s' % (len(region) // 5, row['FishID'][d], row['RegionThreshold'][d]))\n",
    "#         plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "#         plt.axis('off')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out thresholded regions as .zarr files\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out bucket'):\n",
    "    for d, region in tqdm(enumerate(Regions[c]),\n",
    "                          total=len(Regions[c]),\n",
    "                          desc='Saving out regions',\n",
    "                          leave=False):\n",
    "        outputfilename = RegionZarrFiles[c][d].replace('rec_regions',\n",
    "                                                       'rec_regions_thresholded').replace(row.FishID[d],\n",
    "                                                                                          '%s_thresholded_%03d' % (row.FishID[d], row.RegionThreshold[d]))\n",
    "        if not os.path.exists(outputfilename):\n",
    "            print('Writing %s > %s to %s.' % (row.FishID[d],\n",
    "                                              row.RegionThreshold[d],\n",
    "                                              outputfilename[len(Root) + 1:]))\n",
    "            dask.array.to_zarr((region > row.RegionThreshold[d]), outputfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the thresholded regions\n",
    "ThresholdedRegionZarrFiles = [sorted(glob.glob(os.path.join(folder + '_regions_thresholded', '*.zarr'))) for folder in Data['Folder']]\n",
    "ThresholdedRegions = [[dask.array.from_zarr(f) for f in files] for files in ThresholdedRegionZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out a log file for the thresholded files\n",
    "for c, row in tqdm(Data.iterrows(), total=len(Data), desc='Writing log files for regions'):\n",
    "    for d, region in tqdm(enumerate(row.Regions),\n",
    "                          total=len(row.Regions),\n",
    "                          desc=Data.Folder[c][len(Root) + 1:],\n",
    "                          leave=False):\n",
    "        # Generate output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions_thresholded',\n",
    "                                 row['FishID'][d]).replace(row.FishID[d],\n",
    "                                                           '%s_thresholded_%03d' % (row.FishID[d], row.RegionThreshold[d]))\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        # Generate logfile name\n",
    "        logfilename = os.path.join(outputdir, row['FishID'][d] + '.log')\n",
    "        # Delete logfile, if it already exists\n",
    "        if os.path.exists(logfilename):\n",
    "            os.remove(logfilename)\n",
    "        logfile = myLogger(logfilename)\n",
    "        logfile.info('Scan = %s' % os.path.join(row.Sample, row.Scan))\n",
    "        logfile.info('Voxel size = %s um' % row.Voxelsize)\n",
    "        logfile.info('ID = %s' % row['FishID'][d])\n",
    "        logfile.info('Vial = %s' % str(d + 1))\n",
    "        logfile.info('Centroid (x,y) in the original stack = %s, %s' % (int(round(region.centroid[1])), int(round(region.centroid[0]))))\n",
    "        logfile.info('Bounding box (x1:x2, y1:y2) of this region in the original stack = %s:%s, %s:%s' % (region.bbox[1] - buffer, region.bbox[3] + buffer,\n",
    "                                                                                                          region.bbox[0] - buffer, region.bbox[2] + buffer))\n",
    "        logfile.info('Threshold = %s' % row.RegionThreshold[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out one .nrrd file per thresholded region\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out .nrrd file for each thresholded region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(ThresholdedRegions[c]),\n",
    "                            total=len(ThresholdedRegions[c]),\n",
    "                            desc=Data.Folder[c][len(Root) + 1:],\n",
    "                            leave=False):\n",
    "        # Generate output name\n",
    "        outputname = os.path.join(row.Folder + '_regions_thresholded',\n",
    "                                  row['FishID'][d]).replace(row.FishID[d],\n",
    "                                                            '%s_thresholded_%03d.nrrd' % (row.FishID[d], row.RegionThreshold[d]))\n",
    "        # Generate header that goes into the file\n",
    "        # https://pynrrd.readthedocs.io/en/stable/examples.html#example-with-fields-and-custom-fields\n",
    "        header = {'encoding': 'raw',\n",
    "                  'units': ['mm', 'mm', 'mm'],\n",
    "                  'spacings': [row.Voxelsize / 1000, row.Voxelsize / 1000, row.Voxelsize / 1000]}\n",
    "        # Write out file with https://github.com/mhe/pynrrd/\n",
    "        if not os.path.exists(outputname):\n",
    "            nrrd.write(outputname,\n",
    "                       zarrfile.compute().astype('uint8'),\n",
    "                       header,\n",
    "                       index_order='C'  # with c-index order we get the data out with the stack in z-direction\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out thresholded PNG slices \n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out PNGs for each thresholded region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(ThresholdedRegions[c]),\n",
    "                            total=len(ThresholdedRegions[c]),\n",
    "                            desc=Data.Folder[c][len(Root) + 1:],\n",
    "                            leave=False):\n",
    "        # Make output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions_thresholded', row['FishID'][d]).replace(row.FishID[d],'%s_thresholded_%03d' % (row.FishID[d], row.RegionThreshold[d]))\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        # Write threshold value to file names\n",
    "\n",
    "\n",
    "        outputfilenames = [os.path.join(outputdir, os.path.basename(fn)\n",
    "                                        .replace(Data.Sample[c], Data['FishID'][c][d]))\n",
    "                                        .replace('_rec0', '_thresholded_%03d_rec0' % row.RegionThreshold[d]) for fn in Data['Filenames Reconstructions'][c]]\n",
    "        parallelize = True\n",
    "        if parallelize:\n",
    "            # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing \n",
    "            # It is paramount that the filenames are sorted though!\n",
    "            Parallel(n_jobs=-1)(delayed(imsaver)(zarrfile[slice],\n",
    "                                                 outputfilenames[slice]) for slice in range(len(outputfilenames)))\n",
    "        else:\n",
    "            for slice in tqdm(range(len(outputfilenames)),\n",
    "                              total=len(outputfilenames),\n",
    "                              desc='%s' % os.path.splitext(RegionZarrFiles[c][d])[0][len(Root) + 1:],\n",
    "                              leave=False):\n",
    "                if not os.path.exists(outputfilenames[slice]):\n",
    "                    imageio.imwrite(outputfilenames[slice], zarrfile[slice].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d\n",
    "import math\n",
    "import numpy as np\n",
    "from k3d.colormaps import matplotlib_color_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichbucket = 3\n",
    "whichfish = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Displaying fish %s (region %s from bucket %s) below' % (Data['FishID'][whichbucket][whichfish],\n",
    "                                                               whichfish + 1,\n",
    "                                                               Data['Bucket'][whichbucket]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentfish = Regions[whichbucket][whichfish][::subsample, ::subsample, ::subsample].astype(np.float16).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fish with correct bounds: https://github.com/K3D-tools/K3D-jupyter/issues/417#issuecomment-1557778798\n",
    "fish = k3d.volume(currentfish,\n",
    "                  bounds=[0, Data['Voxelsize'][whichbucket] * currentfish.shape[2],\n",
    "                          0, Data['Voxelsize'][whichbucket] * currentfish.shape[1],\n",
    "                          0, Data['Voxelsize'][whichbucket] * currentfish.shape[0]])\n",
    "plot = k3d.plot()\n",
    "plot += fish\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentfish_thresholded = ThresholdedRegions[whichbucket][whichfish][::subsample, ::subsample, ::subsample].astype(np.float16).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fish with correct bounds: https://github.com/K3D-tools/K3D-jupyter/issues/417#issuecomment-1557778798\n",
    "thresholdedfish = k3d.volume(currentfish_thresholded,\n",
    "                             bounds=[0, Data['Voxelsize'][whichbucket] * currentfish_thresholded.shape[2],\n",
    "                                     0, Data['Voxelsize'][whichbucket] * currentfish_thresholded.shape[1],\n",
    "                                     0, Data['Voxelsize'][whichbucket] * currentfish_thresholded.shape[0]],\n",
    "                             color_map=matplotlib_color_maps.Bone\n",
    "                             )\n",
    "plot = k3d.plot()\n",
    "plot += thresholdedfish\n",
    "plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set nice view above and save camera state\n",
    "# https://github.com/K3D-tools/K3D-jupyter/issues/417\n",
    "plot.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out HTML page, the bucket directory\n",
    "outputname = os.path.join(os.path.dirname(Data['Folder'][whichbucket]),\n",
    "                          '%s.3D.html' % (Data['FishID'][whichbucket][whichfish]))\n",
    "if not os.path.exists(outputname):\n",
    "    with open(outputname, \"w\") as f:\n",
    "        plot.camera = [12233.580110967298,\n",
    "                       1495.1256137363334,\n",
    "                       8669.759910429906,\n",
    "                       1567.5184326171875,\n",
    "                       2295.02685546875,\n",
    "                       11340.1328125,\n",
    "                       0.018134042199251698,\n",
    "                       -0.9899408456925172,\n",
    "                       0.14031492630187442]\n",
    "        f.write(plot.get_snapshot())\n",
    "    print('3D view saved to %s' % outputname)\n",
    "else:\n",
    "    print('3D view was already saved to %s, not saving it again' % outputname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopThisThingHere=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to label the stack\n",
    "def labeler(stack):\n",
    "    return labeled_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize .zarr files to only fish-extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Regions):\n",
    "    outfilename = RegionZarrFiles[c].replace('_rec.zarr', '.MIPs.png')\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow(region.max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP already saved to %s' % outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of one of the MIPs\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Histograms = [dask.array.histogram(dask.array.array(region),\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for region in Regions]\n",
    "# Actually compute the data and put only h into the dataframe, so we can use it below.\n",
    "# Discard the bins\n",
    "Histograms = [h.compute() for h, b in Histograms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholds = [skimage.filters.threshold_otsu(region[:,:,:,0][region[:,:,:,0]>10].compute()) for region in Regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, hist in enumerate(Histograms):\n",
    "    plt.semilogy(hist,\n",
    "                 c=seaborn.color_palette()[c])\n",
    "    plt.axvline(Thresholds[c],\n",
    "                label='R%s: %s' % (c, Thresholds[c]),\n",
    "                c=seaborn.color_palette()[c])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Regions):\n",
    "    outfilename = RegionZarrFiles[c].replace('_rec.zarr', '.MIPsasdfasdfa.png')\n",
    "    region = region[:,:,:,0].compute()\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region>Thresholds[c]).max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        # plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP already saved to %s' % outfilename[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = skimage.morphology.label(Regions[0][:,:,:,0]>Thresholds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label fish and save out as .zarr\n",
    "os.makedirs(Data.Folder[whichscan] + '_labeled', exist_ok=True)\n",
    "for c, region in tqdm(enumerate(Regions), total=len(regions)):\n",
    "    plt.subplot(1, 6, c+1)\n",
    "    currentregion = skimage.morphology.label(region[:,:,:,0]>Thresholds[c])\n",
    "    outputfilename = os.path.join(Data.Folder[whichscan] + '_labeled', 'region_%s_rec_labeled.zarr' % str(c+1))\n",
    "    if not os.path.exists(outputfilename):\n",
    "        print('writing to', outputfilename)\n",
    "        zarr_out_3D_convenient = zarr.save(outputfilename, currentregion)\n",
    "    else:\n",
    "        print(outputfilename[len(Root) + 1:], 'already exists')\n",
    "    currentmip = currentregion.max(axis=0)\n",
    "    plt.imshow(currentmip)\n",
    "    plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "    plt.title('Region %s' % c)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in labels again\n",
    "LabelZarrFiles = sorted(glob.glob(os.path.join(Data.Folder[whichscan] + '_labeled', '*.zarr')))\n",
    "Labels = [dask.array.from_zarr(file) for file in LabelZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Labels):\n",
    "    outfilename = LabelZarrFiles[c].replace('_rec_labeled.zarr', '.MIPs.labeled.png')\n",
    "    print(outfilename)\n",
    "    # region = region[:,:,:,0].compute()\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region).max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP overview image already saved to %s' % outfilename[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Labels):\n",
    "    outfilename = LabelZarrFiles[c].replace('_rec_labeled.zarr', '.Summed.labeled.png')\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region).sum(axis=d))\n",
    "            plt.title('Region %s\\n%s Sum' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Summed image already saved to %s' % outfilename[len(Root) + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 333\n",
    "for c, r in enumerate(Regions):\n",
    "    plt.subplot(2,3,c+1)\n",
    "    plt.imshow(r[slice])\n",
    "    # plt.imshow((r[:,:,:,0]>Thresholds[c])[slice], alpha=0.5, cmap='viridis')\n",
    "    plt.imshow(skimage.morphology.label(r[:,:,:,0][slice]>Thresholds[c]), alpha=0.5, cmap='viridis')\n",
    "    plt.title('R%s' % c)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out PNG slices for later use\n",
    "for c, zarrfile in tqdm(enumerate(Labels),\n",
    "                        total=len(Labels),\n",
    "                        desc=Data.Folder[whichscan][len(Root) + 1:]):\n",
    "    # Make output directory\n",
    "    os.makedirs(os.path.splitext(LabelZarrFiles[c])[0], exist_ok=True)\n",
    "    for d, slice in tqdm(enumerate(zarrfile),\n",
    "                         total=len(zarrfile),\n",
    "                         desc='Saving to %s' % os.path.splitext(LabelZarrFiles[c])[0][len(Root) + 1:],\n",
    "                         leave=False):\n",
    "        outfilepath = os.path.join(os.path.splitext(LabelZarrFiles[c])[0],\n",
    "                                   os.path.basename(Data['Filenames Reconstructions'][whichscan][d])).replace('_rec00', '_region_%s_labeled_rec00' % str(c+1))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # plt.imshow(slice.compute())\n",
    "            # plt.show()\n",
    "            # print(type(slice))\n",
    "            imageio.imwrite(outfilepath, slice.compute().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = skimage.feature.blob_dog(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(clean)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
