{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the scans\n",
    "\n",
    "We scanned multiple fish in [a special sample holder](https://github.com/TomoGraphics/Hol3Drs/blob/master/STL/Stickleback.Multiple.stl).\n",
    "This notebook is used to separate them into different bunch of reonstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below are used to set up the whole notebook.\n",
    "They load needed libraries and set some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we need\n",
    "import platform\n",
    "import os\n",
    "import glob\n",
    "import pandas\n",
    "import imageio\n",
    "import numpy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn\n",
    "import dask\n",
    "import dask_image.imread\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import skimage\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our own log file parsing code\n",
    "# This is loaded as a submodule to alleviate excessive copy-pasting between *all* projects we do\n",
    "# See https://github.com/habi/BrukerSkyScanLogfileRuminator for details on its inner workings\n",
    "from BrukerSkyScanLogfileRuminator.parsing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dask temporary folder\n",
    "# Do this before creating a client: https://stackoverflow.com/a/62804525/323100\n",
    "# We use the fast internal SSD for speed reasons\n",
    "import tempfile\n",
    "if 'Linux' in platform.system():\n",
    "    # Check if me mounted the FastSSD, otherwise go to standard tmp file\n",
    "    if os.path.exists(os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')):\n",
    "        tmp = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD', 'tmp')\n",
    "    else:\n",
    "        tmp = tempfile.gettempdir()\n",
    "elif 'Darwin' in platform.system():\n",
    "    tmp = tempfile.gettempdir()\n",
    "else:\n",
    "    if 'anaklin' in platform.node():\n",
    "        tmp = os.path.join('F:\\\\tmp')\n",
    "    else:\n",
    "        tmp = os.path.join('D:\\\\tmp')\n",
    "dask.config.set({'temporary_directory': tmp})\n",
    "print('Dask temporary files go to %s' % dask.config.get('temporary_directory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure defaults\n",
    "plt.rc('image', cmap='gray', interpolation='nearest')  # Display all images in b&w and with 'nearest' interpolation\n",
    "# plt.rcParams['figure.figsize'] = (16, 9)  # Size up figures a bit\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup scale bar defaults\n",
    "plt.rcParams['scalebar.location'] = 'lower right'\n",
    "plt.rcParams['scalebar.frameon'] = False\n",
    "plt.rcParams['scalebar.color'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots identically\n",
    "lines = 3\n",
    "# And then do something like\n",
    "# plt.subplot(lines, int(numpy.ceil(len(Data) / float(lines))), c + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the (tomographic) data can reside on different drives we set a folder to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different locations if running either on Linux or Windows\n",
    "FastSSD = True\n",
    "if 'Linux' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join(os.sep, 'media', 'habi', 'Fast_SSD')\n",
    "    else:\n",
    "        BasePath = os.path.join(os.path.sep, 'home', 'habi', '2214')\n",
    "elif 'Windows' in platform.system():\n",
    "    if FastSSD:\n",
    "        BasePath = os.path.join('F:\\\\')\n",
    "    else:\n",
    "        BasePath = os.path.join('N:\\\\')\n",
    "Root = os.path.join(BasePath, 'IEE Stickleback')\n",
    "print('We are loading all the data from %s' % Root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are set up, actually start to load/ingest the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make us a dataframe for saving all that we need\n",
    "Data = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get *all* log files\n",
    "# Using os.walk is way faster than using recursive glob.glob, see DataWrangling.ipynb for details\n",
    "# Not sorting the found logfiles is also making it quicker\n",
    "Data['LogFile'] = [os.path.join(root, name)\n",
    "                   for root, dirs, files in os.walk(Root)\n",
    "                   for name in files\n",
    "                   if name.endswith((\".log\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all folders\n",
    "Data['Folder'] = [os.path.dirname(f) for f in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a (small) sampler of the loaded data as a first check\n",
    "Data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the logfiles from all the folders that might be on disk but that we don't want to load the data from\n",
    "for c, row in Data.iterrows():\n",
    "    # Since this notebook only deals with the 'BucketOfFish' scans, drop all others\n",
    "    if 'Bucket' not in row.Folder:  # Only use the scans named Bucket* here\n",
    "        Data.drop([c], inplace=True)    \n",
    "    # TEMPORARY GET RID OF D\n",
    "    # TEMPORARY GET RID OF D\n",
    "    # TEMPORARY GET RID OF D\n",
    "    # TEMPORARY GET RID OF D    \n",
    "    # elif '_D' in row.Folder:  # Only look at logs in any rec folder\n",
    "    #     Data.drop([c], inplace=True)        \n",
    "    # TEMPORARY GET RID OF D\n",
    "    # TEMPORARY GET RID OF D\n",
    "    # TEMPORARY GET RID OF D    \n",
    "    elif 'rec' not in row.Folder:  # Only look at logs in any rec folder\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif '_regions' in row.Folder:  # Exclude all log files that we write in this notebook (to $scan$_region folders)\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'SubScan' in row.Folder:  # Exclude any log files from rsyncing temporary data\n",
    "        Data.drop([c], inplace=True)\n",
    "    elif 'rectmp.log' in row.LogFile:  # Exclude any log files from rsyncing temporary data\n",
    "        Data.drop([c], inplace=True)\n",
    "# Reset dataframe to something that we would get if we only would have loaded the 'rec' files\n",
    "Data = Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate us some meaningful colums in the dataframe\n",
    "Data['Sample'] = [os.path.basename(l).replace('_rec.log','') for l in Data['LogFile']]\n",
    "Data['Scan'] = [os.path.basename(os.path.dirname(l)) for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quickly show the data from the last loaded scans\n",
    "Data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file names of all the reconstructions of all the scans\n",
    "Data['Filenames Reconstructions'] = [sorted(glob.glob(os.path.join(f, '*rec0*.png'))) for f in Data['Folder']]\n",
    "# How many reconstructions do we have?\n",
    "Data['Number of reconstructions'] = [len(r) for r in Data['Filenames Reconstructions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop samples which have either not been reconstructed yet or of which we deleted the reconstructions with\n",
    "Data = Data[Data['Number of reconstructions'] > 0]\n",
    "# Reset the dataframe count/index for easier indexing afterwards\n",
    "Data.reset_index(drop=True, inplace=True)\n",
    "print('We have %s folders with reconstructions' % (len(Data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters to doublecheck from logfiles\n",
    "Data['Voxelsize'] = [pixelsize(log) for log in Data['LogFile']]\n",
    "Data['Filter'] = [whichfilter(log) for log in Data['LogFile']]\n",
    "Data['Exposuretime'] = [exposuretime(log) for log in Data['LogFile']]\n",
    "Data['Scanner'] = [scanner(log) for log in Data['LogFile']]\n",
    "Data['Averaging'] = [averaging(log) for log in Data['LogFile']]\n",
    "Data['ProjectionSize'] = [projection_size(log) for log in Data['LogFile']]\n",
    "Data['RotationStep'] = [rotationstep(log) for log in Data['LogFile']]\n",
    "Data['Grayvalue'] = [reconstruction_grayvalue(log) for log in Data['LogFile']]\n",
    "Data['RingartefactCorrection'] = [ringremoval(log) for log in Data['LogFile']]\n",
    "Data['BeamHardeningCorrection'] = [beamhardening(log) for log in Data['LogFile']]\n",
    "Data['DefectPixelMasking'] = [defectpixelmasking(log) for log in Data['LogFile']]\n",
    "Data['Scan date'] = [scandate(log) for log in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load all reconstructions DASK arrays\n",
    "# Reconstructions = [dask_image.imread.imread(os.path.join(folder,'*rec*.png')) for folder in Data['Folder']]\n",
    "# Load all reconstructions into ephemereal DASK arrays, with a nice progress bar...\n",
    "Reconstructions = [None] * len(Data)\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   desc='Loading reconstructions',\n",
    "                   total=len(Data)):\n",
    "    Reconstructions[c] = dask_image.imread.imread(os.path.join(row['Folder'], '*rec*.png'))#[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bucket name\n",
    "Data['Bucket'] = [(l).split(os.sep)[-3].split('_')[-1] for l in Data['LogFile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three cardinal directions\n",
    "# Names adapted to fishes: https://en.wikipedia.org/wiki/Fish_anatomy#Body\n",
    "directions = ['Anteroposterior',\n",
    "              'Lateral',\n",
    "              'Dorsoventral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or calculate the directional MIPs, put them into the dataframe and save them to disk\n",
    "for d, direction in enumerate(directions):\n",
    "    Data['MIP_' + direction] = ''\n",
    "for c, row in tqdm(Data.iterrows(), desc='Working on MIPs', total=len(Data)):\n",
    "    for d, direction in tqdm(enumerate(directions),\n",
    "                             desc='%s/%s' % (row['Sample'], row['Scan']),\n",
    "                             leave=False,\n",
    "                             total=len(directions)):\n",
    "        outfilepath = os.path.join(os.path.dirname(row['Folder']),\n",
    "                                   '%s.%s.MIP.%s.png' % (row['Sample'], row['Scan'], direction))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # Generate MIP\n",
    "            # drop last axis, since dask.imread insists on reading reconstructions PNGs as RGB\n",
    "            mip = Reconstructions[c][:,:,:,0].max(axis=d).compute()\n",
    "            imageio.imwrite(outfilepath, mip.astype('uint8'))\n",
    "        Data.at[c, 'MIP_' + direction] = dask_image.imread.imread(outfilepath).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestCC(segmentation):\n",
    "    # Based on https://stackoverflow.com/a/55110923\n",
    "    labels = skimage.measure.label(segmentation)\n",
    "    assert( labels.max() != 0 ) # assume at least 1 CC\n",
    "    largestCC = labels == numpy.argmax(numpy.bincount(labels.flat)[1:])+1\n",
    "    return largestCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vial_label_extractor(whichscan, threshold=33, part=333, verbose=True):\n",
    "    # Let's get out the numbers, they are 'hidden' in the lower part\n",
    "    bottom_mip = Reconstructions[whichscan][:part].max(axis=0)[:,:,0].compute()\n",
    "    if not threshold:\n",
    "        # Calculate multi Otsu with three classes, use highest threshold\n",
    "        threshold = skimage.filters.threshold_multiotsu(bottom_mip)[-1]\n",
    "    # remove largest component from thresholded bottom MIP\n",
    "    # The largest component are the separation walls of the bucket\n",
    "    numbers = numpy.bitwise_xor(bottom_mip>threshold, getLargestCC(bottom_mip>threshold))\n",
    "    # Clean up the image by removing small objects\n",
    "    numbers_cleaned = skimage.morphology.remove_small_objects(numbers, min_size=10000)\n",
    "    # only labels should remain\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(Data['MIP_Dorsoventral'][whichscan])\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.gca().add_artist(matplotlib.patches.Rectangle((0, 0), Data['MIP_Dorsoventral'][whichscan].shape[1], part, alpha=0.618))\n",
    "        plt.title('Bucket %s' % Data['Bucket'][whichscan])\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(Data['MIP_Anteroposterior'][whichscan])\n",
    "        plt.imshow(numpy.ma.masked_equal(numbers_cleaned, 0), cmap='viridis_r', alpha=0.618)\n",
    "        plt.title('MIP of marked region\\n%s recs >%s - their largest CC' % (part, threshold))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.savefig('%s.%s.Labels.Extracted.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "        plt.show()\n",
    "    return(numbers_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['VialLabels'] = [vial_label_extractor(i, verbose=True) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fish_position(whichscan, threshold=None, verbose=False):\n",
    "    import matplotlib.patches\n",
    "    # Detect the fish positions based on blobs in the top-down MIP\n",
    "    regions = None\n",
    "    td_mip = Data['MIP_Anteroposterior'][whichscan].compute()\n",
    "    if not threshold:\n",
    "        threshold = skimage.filters.threshold_otsu(td_mip[td_mip>10])\n",
    "    td_mip_thresholded = td_mip>threshold\n",
    "    # Remove central part, on some scans the connector shows up...\n",
    "    region_radius=200\n",
    "    td_mip_thresholded[td_mip_thresholded.shape[0]//2-region_radius:td_mip_thresholded.shape[0]//2+region_radius,\n",
    "                       td_mip_thresholded.shape[1]//2-region_radius:td_mip_thresholded.shape[1]//2+region_radius] = 0\n",
    "    # Clean speckles, assuming all fish are larger than 5000 px\n",
    "    cleaned = skimage.morphology.remove_small_objects(td_mip_thresholded,\n",
    "                                                      min_size=5000)\n",
    "    # Remove central part, on some scans the connector shows up...\n",
    "    region_radius=275\n",
    "    cleaned[cleaned.shape[0]//2-region_radius:cleaned.shape[0]//2+region_radius,cleaned.shape[1]//2-region_radius:cleaned.shape[1]//2+region_radius] = 0\n",
    "    # Label image\n",
    "    label_image = skimage.measure.label(cleaned)\n",
    "    # Detect regions\n",
    "    regions = skimage.measure.regionprops(label_image)\n",
    "    if verbose:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(td_mip)\n",
    "        plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], 0), cmap='viridis_r', alpha=0.618)        \n",
    "        plt.title('Bucket %s' % Data['Bucket'][whichscan])\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.subplot(122)\n",
    "        # to make the background transparent, pass the value of `bg_label`,\n",
    "        # and leave `bg_color` as `None` and `kind` as `overlay`   \n",
    "        plt.imshow(skimage.color.label2rgb(label_image, image=td_mip, bg_label=0))\n",
    "        for c, region in enumerate(regions):\n",
    "            # draw rectangle around segmented fish\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                      fill=False, edgecolor='white', ls='--')\n",
    "            # plt.scatter(region.centroid[1], region.centroid[0])\n",
    "            plt.gca().add_patch(rect)\n",
    "            plt.annotate('%s' % region.label, xy=((minc + maxc) / 2, minr - 15), color='white', ha='center')\n",
    "        plt.title('Detected fish')\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "        plt.axis('off')\n",
    "        plt.savefig('%s.%s.Labels.Detected.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))        \n",
    "        plt.show()\n",
    "    return(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the region properties of the single fish\n",
    "Data['Regions'] = [detect_fish_position(i, verbose=True) for i in range(len(Data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_list(list, neworder = [0, 2, 4, 5, 3, 1]):\n",
    "    '''\n",
    "    Shuffle a list to a defined order\n",
    "    We *deliberately* want a new list, so we can keep the old one around for double-checks\n",
    "    https://stackoverflow.com/questions/2177590/how-can-i-reorder-a-list#comment106984501_2177607\n",
    "    '''\n",
    "    ordered_list = [list[i] for i in neworder]\n",
    "    return(ordered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had a *very* hard time to correctly shuffle the list\n",
    "# If we both print out the label from the image and the shuffeled list, it's easier to check :)\n",
    "print('X: from image', [i+1 for i in range(6)])\n",
    "# For most buckets\n",
    "print('Y: from label', reorder_list([i+1 for i in range(6)]))\n",
    "# For bucket C\n",
    "print('C: from label', reorder_list([i+1 for i in range(6)], neworder = [0, 2, 3, 5, 4, 1]))\n",
    "# For bucket D\n",
    "print('D: from label', reorder_list([i+1 for i in range(6)], neworder = [0, 2, 3, 5, 4, 1]))\n",
    "# For bucket E\n",
    "print('E: from label', reorder_list([i+1 for i in range(6)], neworder = [1, 3, 5, 4, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder regions\n",
    "# Since we always reconstructed vial 1 up north, the order is consistent (most of the time)\n",
    "# We can thus just reshuffle *all* the detected regions for writing them out correctly afterwards\n",
    "Data['Regions_Ordered'] = [reorder_list(regions) for regions in Data['Regions']]\n",
    "# WARNING the region.labels now do NOT correspond to the correct label anymore :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the reshuffling above does not work for one scan, we can reorder it deliberately again.\n",
    "# Bucket C: label 4 and 5 are swapped, due to one having a bigger fish than the others\n",
    "Data['Regions_Ordered'][2] = reorder_list(Data['Regions'][2], neworder = [0, 2, 3, 5, 4, 1])\n",
    "# Bucket D: label 4 and 5 are swapped, due to one having a bigger fish than the others\n",
    "Data['Regions_Ordered'][3] = reorder_list(Data['Regions'][3], neworder = [0, 2, 3, 5, 4, 1])\n",
    "# Bucket E: we (temporarily) reconstructed 6 north\n",
    "Data['Regions_Ordered'][4] = reorder_list(Data['Regions'][4], neworder = [1, 3, 5, 4, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct us a fish number\n",
    "Data['FishNumber'] = [[reg.label + (len(region) * c - 1) for reg in region] for c, region in enumerate(Data.Regions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite blunders in first two batches with their correct numbers\n",
    "# Bucket A\n",
    "Data['FishNumber'][0] = [fn + 1 for fn in range(len(Data['FishNumber'][0]))]\n",
    "Data['FishNumber'][0][2] = 30\n",
    "# Bucket B\n",
    "Data['FishNumber'][1][0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct us a fish ID\n",
    "# This should correspond to the fish ID in Bens XLS sheet\n",
    "Data['FishID'] = [['FG.X23.%03d' % n for n in number] for number in Data.FishNumber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doublecheck_fish_position(whichscan):\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(Data['MIP_Anteroposterior'][whichscan])\n",
    "    plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], False), cmap='viridis_r')\n",
    "    for c, region in enumerate(Data['Regions'][whichscan]):\n",
    "        plt.annotate('%s' % str(c+1),\n",
    "                     xy=(region.centroid[1] + 100, region.centroid[0]),\n",
    "                     color='black',\n",
    "                     va='center',\n",
    "                     bbox=dict(fc=\"white\", alpha=0.618))\n",
    "    plt.title('Bucket %s, calculated labels' % Data['Bucket'][whichscan])\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "    plt.axis('off')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(Data['MIP_Anteroposterior'][whichscan])\n",
    "    plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan], False), cmap='viridis_r')\n",
    "    for c, region in enumerate(Data['Regions_Ordered'][whichscan]):\n",
    "        plt.annotate('%s:%s' % (c+1, Data['FishID'][whichscan][c]),                     \n",
    "                     xy=(region.centroid[1], region.centroid[0]),\n",
    "                     color='black',\n",
    "                     fontsize=8,\n",
    "                     va='center',\n",
    "                     bbox=dict(fc=\"white\", alpha=0.618))\n",
    "    plt.title('Resorted label:mapped IDs')\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "    plt.axis('off')\n",
    "    plt.savefig('%s.%s.Labels.Sorted.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan]), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Data)):\n",
    "    doublecheck_fish_position(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regionextractor(whichscan, buffer=50, verbose=True):\n",
    "    os.makedirs(Data.Folder[whichscan] + '_regions', exist_ok=True)\n",
    "    for c, region in tqdm(enumerate(Data['Regions_Ordered'][whichscan]),\n",
    "                          total=len(Data['Regions_Ordered'][whichscan]),\n",
    "                          desc='Extracting and visualizing regions'):\n",
    "        outputfilename = os.path.join(Data.Folder[whichscan] + '_regions', 'region_%s_%s.zarr' % (str(c+1),\n",
    "                                                                                                  Data['FishID'][whichscan][c]))\n",
    "        if not os.path.exists(outputfilename):\n",
    "            # Crop current region out of reconstructions stack, drop RGB axis and rechunk, making for even more efficient access\n",
    "            currentregion = Reconstructions[whichscan][:,region.bbox[0]-buffer:region.bbox[2]+buffer,region.bbox[1]-buffer:region.bbox[3]+buffer][:,:,:,0].rechunk('auto')\n",
    "            print('Writing to %s. This takes a while.' % outputfilename[len(Root)+1:])\n",
    "            dask.array.to_zarr(currentregion, outputfilename)\n",
    "        if verbose:\n",
    "            # Read written file back in, so we can profit from the rechunking\n",
    "            currentregion = dask.array.from_zarr(outputfilename)\n",
    "            plt.subplot(2,6,c+1)\n",
    "            plt.imshow(Data['MIP_Anteroposterior'][whichscan][region.bbox[0]-buffer:region.bbox[2]+buffer,region.bbox[1]-buffer:region.bbox[3]+buffer])\n",
    "            plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][whichscan][region.bbox[0]-buffer:region.bbox[2]+buffer,region.bbox[1]-buffer:region.bbox[3]+buffer],\n",
    "                                             False),\n",
    "                       cmap='viridis_r')\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "            plt.title('%s:Cut original' % str(c + 1))\n",
    "            plt.axis('off')\n",
    "            # plt.subplot(6, 2, (2 * c ) + 2)\n",
    "            plt.subplot(2,6,c+1+6)  \n",
    "            # Recalculate MIP for double-checking\n",
    "            plt.imshow(currentregion.max(axis=0))\n",
    "            plt.gca().add_artist(ScaleBar(Data['Voxelsize'][whichscan], 'um'))\n",
    "            plt.title(Data['FishID'][whichscan][c])\n",
    "            plt.axis('off')\n",
    "    if verbose:\n",
    "        plt.savefig('%s.%s.Regions.png' % (os.path.join(os.path.dirname(Data['Folder'][whichscan] + '_regions'), Data.Sample[whichscan]), Data.Scan[whichscan]))\n",
    "        plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly state the buffer, we want it later for adding the crop region to the regional log files\n",
    "buffer = 50\n",
    "for i in range(len(Data)):\n",
    "    regionextractor(i, buffer=buffer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load overview and labbook image, if present\n",
    "Data['LabbookImage'] = [dask_image.imread.imread(os.path.join(os.path.dirname(f), '_labbook.jpg')).squeeze() for f in Data['Folder']]\n",
    "Data['OverviewImage'] = [dask_image.imread.imread(os.path.join(os.path.dirname(f), '_overview.jpg')).squeeze()\n",
    "                         if os.path.exists(os.path.join(os.path.dirname(f), '_overview.jpg'))\n",
    "                         else numpy.random.random((2**8,2**8)) for f in Data['Folder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all information we have, to double-check the mapping\n",
    "for c, row in Data.iterrows():\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(row.LabbookImage)\n",
    "    plt.title('Bucket %s: Labbook' % row.Bucket)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(row.OverviewImage)\n",
    "    plt.title('Bucket %s: Tubes' % row.Bucket)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(Data['MIP_Anteroposterior'][c])\n",
    "    plt.imshow(numpy.ma.masked_equal(Data['VialLabels'][c], False), cmap='viridis_r')\n",
    "    for d, region in enumerate(Data['Regions_Ordered'][c]):\n",
    "        plt.annotate('%s:%s' % (d+1, Data['FishID'][c][d]),                     \n",
    "                     xy=(region.centroid[1], region.centroid[0]),\n",
    "                     color='black',\n",
    "                     ha='center',\n",
    "                     va='center',\n",
    "                     fontsize=8,\n",
    "                     bbox=dict(fc=\"white\", alpha=0.618))\n",
    "    plt.title('Bucket %s: IDs' % row.Bucket)\n",
    "    plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))\n",
    "    plt.axis('off')\n",
    "    plt.savefig('%s.%s.Mapping.png' % (os.path.join(os.path.dirname(row['Folder']), row.Sample), row.Scan))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in regions again\n",
    "RegionZarrFiles = [glob.glob(os.path.join(folder + '_regions', '*.zarr')) for folder in Data['Folder']]\n",
    "Regions = [[dask.array.from_zarr(f) for f in files] for files in RegionZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to generate log files for the cutout regions\n",
    "# Aeons ago, we wrote a little wrapper function to log stuff at TOMCAT\n",
    "# https://github.com/habi/TOMCAT/blob/master/postscan/StackedScanOverlapFinder.py#L104\n",
    "# The function below is slightly tweaked from there\n",
    "def myLogger(logfilename, verbose=False):\n",
    "    import logging\n",
    "    logger = logging.getLogger(logfilename)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.FileHandler(logfilename, 'w')\n",
    "    logger.addHandler(handler)\n",
    "    if verbose:\n",
    "        print('Logging to %s' % logfilename)\n",
    "    return logger\n",
    "# Then write to the file with\n",
    "# logfile = myLogger(Filename))\n",
    "# logfile.info('Put this into the log file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out a log file\n",
    "for c, row in tqdm(Data.iterrows(), total=len(Data), desc='Writing log files for regions'):\n",
    "    for d, region in tqdm(enumerate(row.Regions),\n",
    "                            total=len(row.Regions),\n",
    "                            desc=Data.Folder[c][len(Root)+1:]):\n",
    "        # Generate output directory\n",
    "        os.makedirs(os.path.join(row.Folder + '_regions', row['FishID'][d]), exist_ok=True)\n",
    "        # Generate logfile name\n",
    "        logfilename = os.path.join(row.Folder + '_regions', row['FishID'][d], row['FishID'][d] + '.log')\n",
    "        # Delete logfile, if it already exists\n",
    "        if os.path.exists(logfilename):\n",
    "            os.remove(logfilename)\n",
    "        logfile = myLogger(logfilename)\n",
    "        logfile.info('Scan = %s' % os.path.join(row.Sample, row.Scan))\n",
    "        logfile.info('Voxel size = %s um' % row.Voxelsize)\n",
    "        logfile.info('ID = %s' % row['FishID'][d])\n",
    "        logfile.info('Vial = %s' % str(d + 1))\n",
    "        logfile.info('Centroid (x,y) in the original stack = %s, %s' % (int(round(region.centroid[1])), int(round(region.centroid[0]))))\n",
    "        logfile.info('Bounding box (x1:x2, y1:y2) of this region in the original stack = %s:%s, %s:%s' % (region.bbox[1]-buffer, region.bbox[3]+buffer,\n",
    "                                                                                                          region.bbox[0]-buffer, region.bbox[2]+buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def imsaver(image, filename):\n",
    "    ''' Function for parallelizing writing out images '''\n",
    "    if not os.path.exists(filename):  # only do something if there's no image on disk\n",
    "        if image.mean():  # only write something if there's really an image\n",
    "            imageio.imwrite(filename, image.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out PNG slices \n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out PNGs for each region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(Regions[c]),\n",
    "                            total=len(Regions[c]),\n",
    "                            desc=Data.Folder[c][len(Root)+1:]):\n",
    "        # print(zarrfile.shape)\n",
    "        # plt.imshow(zarrfile[666])\n",
    "        # plt.show()\n",
    "        # Make output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions', row['FishID'][d])\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        outputfilenames = [os.path.join(outputdir,\n",
    "                                        os.path.basename(fn)).replace(Data.Sample[c], Data['FishID'][c][d]) for fn in Data['Filenames Reconstructions'][c]]\n",
    "        parallelize = True\n",
    "        if parallelize:\n",
    "            # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing \n",
    "            # It is paramount that the filenames are sorted though!\n",
    "            Parallel(n_jobs=-1)(delayed(imsaver)(zarrfile[slice],\n",
    "                                                 outputfilenames[slice]) for slice in range(len(outputfilenames)))\n",
    "        else:\n",
    "            for slice in tqdm(range(len(outputfilenames)),\n",
    "                              total=len(outputfilenames),\n",
    "                              desc='%s' % os.path.splitext(RegionZarrFiles[c][d])[0][len(Root)+1:],\n",
    "                              leave=False):\n",
    "                if not os.path.exists(outputfilenames[slice]):\n",
    "                    imageio.imwrite(outputfilenames[slice], zarrfile[slice].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholder(stack, discard=5, verbose=False):\n",
    "    '''\n",
    "    Threshold function to reliably threshold *only* bones\n",
    "    First, we calculate a low threshold to 'filter' out the fish.\n",
    "    Then, we threshold everything above that again to get the bone threshold\n",
    "    '''\n",
    "    thresholds = skimage.filters.threshold_multiotsu(stack[stack>discard].compute(),\n",
    "                                                     classes=4)\n",
    "    if verbose:\n",
    "        histogram, bins = dask.array.histogram(stack,\n",
    "                                               bins=2**8,\n",
    "                                               range=[0, 2**8])\n",
    "        plt.semilogy(histogram)\n",
    "        plt.axvline(discard, label='completely discarded, below %s' % discard)\n",
    "        for t in thresholds:\n",
    "            plt.axvline(t, label='threshold %s' % t)\n",
    "        plt.xlim([0, 2**8])\n",
    "        plt.legend()\n",
    "        seaborn.despine()\n",
    "        plt.show()\n",
    "    # Return only the middle threshold value\n",
    "    return(thresholds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = thresholder(Regions[0][0], verbose=False)\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 1500\n",
    "plt.subplot(131)\n",
    "plt.imshow(Regions[0][0][slice])\n",
    "plt.axis('off')\n",
    "plt.subplot(132)\n",
    "plt.imshow((Regions[0][0][slice]>thresholds))\n",
    "plt.title(t)    \n",
    "plt.axis('off')\n",
    "plt.subplot(133)\n",
    "plt.imshow(Regions[0][0][slice][150:600,50:-50])\n",
    "plt.imshow(dask.array.ma.masked_equal((Regions[0][0][slice]>thresholds)[150:600,50:-50], 0), cmap='viridis_r', alpha=0.618)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate threshold for each separated region\n",
    "# Each \"threshold\" is actually three values, we select the middle one \"[1]\" later on.\n",
    "Data['RegionThreshold'] = [[thresholder(rg) for rg in regions] for regions in Regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['RegionThreshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, row in Data.iterrows():\n",
    "    print(row.Bucket)\n",
    "    for d, region in enumerate(Regions[c]):\n",
    "        plt.imshow(region[len(region)//5]>row['RegionThreshold'][d] * 2)\n",
    "        plt.title('Slice %s of %s > %s' % (len(region)//5, row['FishID'][d], row['RegionThreshold'][d]))\n",
    "        plt.gca().add_artist(ScaleBar(Data['Voxelsize'][c], 'um'))        \n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out thresholded regions\n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out bucket'):\n",
    "    for d, region in tqdm(enumerate(Regions[c]),\n",
    "                          total=len(Regions[c]),\n",
    "                          desc='Saving out regions',\n",
    "                          leave=False):\n",
    "        outputfilename = RegionZarrFiles[c][d].replace('rec_regions',\n",
    "                                                       'rec_regions_thresholded').replace(row.FishID[d],\n",
    "                                                                                          '%s_thresholded_%03d' % (row.FishID[d], row.RegionThreshold[d]))\n",
    "        if not os.path.exists(outputfilename):\n",
    "            print('Writing %s > %s to %s.' % (row.FishID[d],\n",
    "                                              row.RegionThreshold[d],\n",
    "                                              outputfilename[len(Root)+1:]))\n",
    "            dask.array.to_zarr((region>row.RegionThreshold[d]), outputfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the thresholded regions\n",
    "ThresholdedRegionZarrFiles = [glob.glob(os.path.join(folder + '_regions_thresholded', '*.zarr')) for folder in Data['Folder']]\n",
    "ThresholdedRegions = [[dask.array.from_zarr(f) for f in files] for files in ThresholdedRegionZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out thresholded PNG slices \n",
    "for c, row in tqdm(Data.iterrows(),\n",
    "                   total=len(Data),\n",
    "                   desc='Saving out PNGs for each thresholded region of each bucket'):\n",
    "    for d, zarrfile in tqdm(enumerate(ThresholdedRegions[c]),\n",
    "                            total=len(ThresholdedRegions[c]),\n",
    "                            desc=Data.Folder[c][len(Root)+1:]):\n",
    "        # Make output directory\n",
    "        outputdir = os.path.join(row.Folder + '_regions_thresholded', row['FishID'][d])\n",
    "        os.makedirs(outputdir, exist_ok=True)\n",
    "        outputfilenames = [os.path.join(outputdir,\n",
    "                                        os.path.basename(fn)).replace(Data.Sample[c], Data['FishID'][c][d]) for fn in Data['Filenames Reconstructions'][c]]\n",
    "        parallelize = True\n",
    "        if parallelize:\n",
    "            # Hat tip to Oleksiy for providing a snippet to parallelize the PNG writing \n",
    "            # It is paramount that the filenames are sorted though!\n",
    "            Parallel(n_jobs=-1)(delayed(imsaver)(zarrfile[slice],\n",
    "                                                 outputfilenames[slice]) for slice in range(len(outputfilenames)))\n",
    "        else:\n",
    "            for slice in tqdm(range(len(outputfilenames)),\n",
    "                              total=len(outputfilenames),\n",
    "                              desc='%s' % os.path.splitext(RegionZarrFiles[c][d])[0][len(Root)+1:],\n",
    "                              leave=False):\n",
    "                if not os.path.exists(outputfilenames[slice]):\n",
    "                    imageio.imwrite(outputfilenames[slice], zarrfile[slice].astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(stack):\n",
    "    return(labeled_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSDFASDFASDF==Â£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize .zarr files to only fish-extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Regions):\n",
    "    outfilename = RegionZarrFiles[c].replace('_rec.zarr', '.MIPs.png')\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow(region.max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP already saved to %s' % outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histograms of one of the MIPs\n",
    "# Caveat: dask.da.histogram returns histogram AND bins, making each histogram a 'nested' list of [h, b]\n",
    "Histograms = [dask.array.histogram(dask.array.array(region),\n",
    "                                          bins=2**8,\n",
    "                                          range=[0, 2**8]) for region in Regions]\n",
    "# Actually compute the data and put only h into the dataframe, so we can use it below.\n",
    "# Discard the bins\n",
    "Histograms = [h.compute() for h, b in Histograms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholds = [skimage.filters.threshold_otsu(region[:,:,:,0][region[:,:,:,0]>10].compute()) for region in Regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, hist in enumerate(Histograms):\n",
    "    plt.semilogy(hist,\n",
    "                 c=seaborn.color_palette()[c])\n",
    "    plt.axvline(Thresholds[c],\n",
    "                label='R%s: %s' % (c, Thresholds[c]),\n",
    "                c=seaborn.color_palette()[c])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Regions):\n",
    "    outfilename = RegionZarrFiles[c].replace('_rec.zarr', '.MIPsasdfasdfa.png')\n",
    "    region = region[:,:,:,0].compute()\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region>Thresholds[c]).max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        # plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP already saved to %s' % outfilename[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = skimage.morphology.label(Regions[0][:,:,:,0]>Thresholds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label fish and save out as .zarr\n",
    "os.makedirs(Data.Folder[whichscan] + '_labeled', exist_ok=True)\n",
    "for c, region in tqdm(enumerate(Regions), total=len(regions)):\n",
    "    plt.subplot(1, 6, c+1)\n",
    "    currentregion = skimage.morphology.label(region[:,:,:,0]>Thresholds[c])\n",
    "    outputfilename = os.path.join(Data.Folder[whichscan] + '_labeled', 'region_%s_rec_labeled.zarr' % str(c+1))\n",
    "    if not os.path.exists(outputfilename):\n",
    "        print('writing to', outputfilename)\n",
    "        zarr_out_3D_convenient = zarr.save(outputfilename, currentregion)\n",
    "    else:\n",
    "        print(outputfilename[len(Root)+1:], 'already exists')\n",
    "    currentmip = currentregion.max(axis=0)\n",
    "    plt.imshow(currentmip)\n",
    "    plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "    plt.title('Region %s' % c)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in labels again\n",
    "LabelZarrFiles = glob.glob(os.path.join(Data.Folder[whichscan] + '_labeled', '*.zarr'))\n",
    "Labels = [dask.array.from_zarr(file) for file in LabelZarrFiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Labels):\n",
    "    outfilename = LabelZarrFiles[c].replace('_rec_labeled.zarr', '.MIPs.labeled.png')\n",
    "    print(outfilename)\n",
    "    # region = region[:,:,:,0].compute()\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region).max(axis=d))\n",
    "            plt.title('Region %s\\n%s MIP' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('MIP overview image already saved to %s' % outfilename[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, region in enumerate(Labels):\n",
    "    outfilename = LabelZarrFiles[c].replace('_rec_labeled.zarr', '.Summed.labeled.png')\n",
    "    if not os.path.exists(outfilename):\n",
    "        for d, direction in enumerate(directions):\n",
    "            plt.subplot(1, 3 , d+1)\n",
    "            plt.imshow((region).sum(axis=d))\n",
    "            plt.title('Region %s\\n%s Sum' % (c, direction))\n",
    "            plt.axis('off')\n",
    "            plt.gca().add_artist(ScaleBar(voxelsize, 'um'))\n",
    "        plt.savefig(outfilename)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Summed image already saved to %s' % outfilename[len(Root)+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = 333\n",
    "for c, r in enumerate(Regions):\n",
    "    plt.subplot(2,3,c+1)\n",
    "    plt.imshow(r[slice])\n",
    "    # plt.imshow((r[:,:,:,0]>Thresholds[c])[slice], alpha=0.5, cmap='viridis')\n",
    "    plt.imshow(skimage.morphology.label(r[:,:,:,0][slice]>Thresholds[c]), alpha=0.5, cmap='viridis')\n",
    "    plt.title('R%s' % c)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out PNG slices for later use\n",
    "for c, zarrfile in tqdm(enumerate(Labels),\n",
    "                        total=len(Labels),\n",
    "                        desc=Data.Folder[whichscan][len(Root)+1:]):\n",
    "    # Make output directory\n",
    "    os.makedirs(os.path.splitext(LabelZarrFiles[c])[0], exist_ok=True)\n",
    "    for d, slice in tqdm(enumerate(zarrfile),\n",
    "                         total=len(zarrfile),\n",
    "                         desc='Saving to %s' % os.path.splitext(LabelZarrFiles[c])[0][len(Root)+1:],\n",
    "                         leave=False):\n",
    "        outfilepath = os.path.join(os.path.splitext(LabelZarrFiles[c])[0],\n",
    "                                   os.path.basename(Data['Filenames Reconstructions'][whichscan][d])).replace('_rec00', '_region_%s_labeled_rec00' % str(c+1))\n",
    "        if not os.path.exists(outfilepath):\n",
    "            # plt.imshow(slice.compute())\n",
    "            # plt.show()\n",
    "            # print(type(slice))\n",
    "            imageio.imwrite(outfilepath, slice.compute().astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = skimage.feature.blob_dog(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(clean)\n",
    "plt.subplot(122)\n",
    "plt.imshow(mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
